import os
import sys
import time
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import logging
from stable_baselines3 import DQN
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.evaluation import evaluate_policy

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("sb3_training.log"),
        logging.StreamHandler()
    ]
)

# Ajouter le répertoire parent au chemin pour pouvoir importer les modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from gym_env.tacticiens_env import TacticiensEnv

class TrainingInfoCallback(BaseCallback):
    """
    Callback personnalisé pour suivre et afficher les statistiques d'entraînement
    """
    def __init__(self, eval_env=None, eval_freq=1000, verbose=1):
        super(TrainingInfoCallback, self).__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_wins = []
        self.start_time = time.time()
        self.best_mean_reward = -np.inf
        self.best_model_path = None

    def _on_training_start(self):
        """Appelé au début de l'entraînement"""
        self.start_time = time.time()
        logging.info("Début de l'entraînement")

        # Créer le dossier pour les meilleurs modèles
        if self.eval_env is not None:
            os.makedirs("./best_models", exist_ok=True)

    def _on_step(self):
        """Appelé à chaque étape de l'entraînement"""
        # Collecter les statistiques des épisodes terminés
        for info in self.locals["infos"]:
            if "episode" in info:
                reward = info["episode"]["r"]
                length = info["episode"]["l"]
                is_win = 1 if info.get('win', False) else 0

                self.episode_rewards.append(reward)
                self.episode_lengths.append(length)
                self.episode_wins.append(is_win)

                if self.verbose > 0:
                    logging.debug(f"Episode: reward={reward:.2f}, length={length}, win={is_win}")

        # Afficher des statistiques périodiquement
        if self.n_calls % 1000 == 0:
            # Calculer les statistiques
            elapsed_time = time.time() - self.start_time
            avg_reward = np.mean(self.episode_rewards[-100:]) if self.episode_rewards else 0
            avg_length = np.mean(self.episode_lengths[-100:]) if self.episode_lengths else 0
            win_rate = sum(self.episode_wins[-100:]) / max(len(self.episode_wins[-100:]), 1)

            # Afficher les statistiques
            logging.info(f"Step: {self.num_timesteps} ({elapsed_time:.2f}s)")
            logging.info(f"  Score moyen (100 derniers): {avg_reward:.2f}")
            logging.info(f"  Longueur moyenne (100 derniers): {avg_length:.2f}")
            logging.info(f"  Taux de victoire (100 derniers): {win_rate:.2%}")

            # Évaluer le modèle si un environnement d'évaluation est fourni
            if self.eval_env is not None and self.n_calls % self.eval_freq == 0:
                mean_reward, std_reward = evaluate_policy(
                    self.model,
                    self.eval_env,
                    n_eval_episodes=10,
                    deterministic=True
                )

                logging.info(f"  Évaluation: score={mean_reward:.2f} ± {std_reward:.2f}")

                # Sauvegarder le meilleur modèle
                if mean_reward > self.best_mean_reward:
                    self.best_mean_reward = mean_reward
                    self.best_model_path = f"./best_models/best_model_{self.num_timesteps}_{mean_reward:.2f}.zip"
                    self.model.save(self.best_model_path)
                    logging.info(f"  Nouveau meilleur modèle sauvegardé: {self.best_model_path}")

        return True

    def _on_training_end(self):
        """Appelé à la fin de l'entraînement"""
        # Calculer les statistiques finales
        total_time = time.time() - self.start_time
        total_episodes = len(self.episode_rewards)
        avg_reward = np.mean(self.episode_rewards) if self.episode_rewards else 0
        win_rate = sum(self.episode_wins) / max(len(self.episode_wins), 1)

        # Afficher les statistiques finales
        logging.info(f"Fin de l'entraînement: {total_time:.2f}s, {total_episodes} épisodes")
        logging.info(f"Score moyen: {avg_reward:.2f}")
        logging.info(f"Taux de victoire: {win_rate:.2%}")

        if self.best_model_path:
            logging.info(f"Meilleur modèle: {self.best_model_path} (score: {self.best_mean_reward:.2f})")

        # Tracer les courbes d'apprentissage
        self._plot_learning_curves()

    def _plot_learning_curves(self):
        """Trace et sauvegarde les courbes d'apprentissage"""
        if not self.episode_rewards:
            return

        plt.figure(figsize=(15, 10))

        # Récompenses
        plt.subplot(2, 2, 1)
        plt.plot(self.episode_rewards)
        plt.title('Récompenses par épisode')
        plt.xlabel('Épisode')
        plt.ylabel('Récompense')

        # Moyenne mobile des récompenses
        plt.subplot(2, 2, 2)
        window = min(100, len(self.episode_rewards))
        if window > 0:
            rolling_mean = np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')
            plt.plot(rolling_mean)
            plt.title(f'Moyenne mobile des récompenses (fenêtre: {window})')
            plt.xlabel('Épisode')
            plt.ylabel('Récompense moyenne')

        # Longueurs des épisodes
        plt.subplot(2, 2, 3)
        plt.plot(self.episode_lengths)
        plt.title('Longueur des épisodes')
        plt.xlabel('Épisode')
        plt.ylabel('Nombre d\'étapes')

        # Taux de victoire
        plt.subplot(2, 2, 4)
        window = min(100, len(self.episode_wins))
        if window > 0:
            rolling_win_rate = np.convolve(self.episode_wins, np.ones(window)/window, mode='valid')
            plt.plot(rolling_win_rate)
            plt.title(f'Taux de victoire (fenêtre: {window})')
            plt.xlabel('Épisode')
            plt.ylabel('Taux de victoire')

        plt.tight_layout()
        plt.savefig("./learning_curves_sb3.png")
        logging.info("Courbes d'apprentissage sauvegardées dans ./learning_curves_sb3.png")

def train_sb3_dqn(timesteps=10000, save_path="./models/sb3_dqn", config=None):
    """
    Entraîne un agent DQN avec Stable Baselines 3

    Args:
        timesteps: Nombre total d'étapes d'entraînement
        save_path: Chemin pour sauvegarder les modèles
        config: Dictionnaire de configuration (optionnel)

    Returns:
        model: Le modèle entraîné
    """
    # Paramètres par défaut
    config = config or {}
    learning_rate = config.get('learning_rate', 1e-3)
    buffer_size = config.get('buffer_size', 10000)
    learning_starts = config.get('learning_starts', 1000)
    batch_size = config.get('batch_size', 32)
    gamma = config.get('gamma', 0.99)
    target_update_interval = config.get('target_update_interval', 500)
    train_freq = config.get('train_freq', 10)
    gradient_steps = config.get('gradient_steps', 1)
    exploration_fraction = config.get('exploration_fraction', 0.1)
    exploration_final_eps = config.get('exploration_final_eps', 0.05)

    # Créer les dossiers pour sauvegarder les modèles
    os.makedirs(save_path, exist_ok=True)

    # Créer et vérifier l'environnement
    env = TacticiensEnv(opponent_type='random')
    try:
        check_env(env, warn=True)
        logging.info("Vérification de l'environnement réussie")
    except Exception as e:
        logging.warning(f"Vérification de l'environnement: {e}")

    # Créer l'environnement d'entraînement avec Monitor
    train_env = Monitor(TacticiensEnv(opponent_type='random'))

    # Créer l'environnement d'évaluation
    eval_env = Monitor(TacticiensEnv(opponent_type='random'))

    # Créer le modèle DQN
    model = DQN(
        "MlpPolicy",
        train_env,
        learning_rate=learning_rate,
        buffer_size=buffer_size,
        learning_starts=learning_starts,
        batch_size=batch_size,
        gamma=gamma,
        target_update_interval=target_update_interval,
        train_freq=train_freq,
        gradient_steps=gradient_steps,
        exploration_fraction=exploration_fraction,
        exploration_final_eps=exploration_final_eps,
        verbose=1
    )

    # Créer les callbacks
    checkpoint_callback = CheckpointCallback(
        save_freq=1000,
        save_path=save_path,
        name_prefix="dqn_model"
    )

    training_info_callback = TrainingInfoCallback(
        eval_env=eval_env,
        eval_freq=5000,
        verbose=1
    )

    # Entraîner le modèle
    start_time = time.time()
    logging.info(f"Début de l'entraînement pour {timesteps} étapes")

    model.learn(
        total_timesteps=timesteps,
        callback=[checkpoint_callback, training_info_callback],
        log_interval=1000
    )

    # Calculer le temps d'entraînement
    training_time = time.time() - start_time
    logging.info(f"Entraînement terminé en {training_time:.2f} secondes")

    # Sauvegarder le modèle final
    final_model_path = os.path.join(save_path, "dqn_final_model")
    model.save(final_model_path)
    logging.info(f"Modèle final sauvegardé dans {final_model_path}")

    # Fermer les environnements
    train_env.close()
    eval_env.close()

    return model

if __name__ == "__main__":
    # Définir les paramètres d'entraînement
    timesteps = 10000
    save_path = "./models/sb3_dqn"

    # Configuration personnalisée
    config = {
        'learning_rate': 5e-4,
        'buffer_size': 50000,
        'learning_starts': 1000,
        'batch_size': 64,
        'gamma': 0.99,
        'target_update_interval': 1000,
        'train_freq': 4,
        'gradient_steps': 1,
        'exploration_fraction': 0.2,
        'exploration_final_eps': 0.05
    }

    # Entraîner l'agent
    logging.info(f"Démarrage de l'entraînement SB3 DQN pour {timesteps} étapes")
    model = train_sb3_dqn(timesteps, save_path, config)
    logging.info("Entraînement SB3 DQN terminé")