import os
import sys
import time
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3 import DQN
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.env_checker import check_env

# Ajouter le répertoire parent au chemin pour pouvoir importer les modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from gym_env.tacticiens_env import TacticiensEnv

# Vérifier l'environnement
env = TacticiensEnv(opponent_type='random')
check_env(env, warn=True)

class TrainingInfoCallback(BaseCallback):
    """
    Callback personnalisé pour afficher des informations pendant l'entraînement.
    """
    def __init__(self, verbose=1):
        super(TrainingInfoCallback, self).__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_wins = []  # Liste pour stocker les victoires (1 pour victoire, 0 sinon)
        self.start_time = time.time()
        self.best_reward = -float('inf')
        self.best_model_path = None

    def _on_step(self) -> bool:
        # Collecter les récompenses et longueurs des épisodes
        for info in self.locals["infos"]:
            if "episode" in info:
                reward = info["episode"]["r"]
                length = info["episode"]["l"]
                is_win = 1 if info.get('win', False) else 0
                self.episode_rewards.append(reward)
                self.episode_lengths.append(length)
                self.episode_wins.append(is_win)

        # Afficher des informations toutes les 1000 étapes
        if self.n_calls % 1000 == 0:
            elapsed_time = time.time() - self.start_time
            avg_reward = np.mean(self.episode_rewards[-100:]) if self.episode_rewards else 0
            avg_moves = np.mean(self.episode_lengths[-100:]) if self.episode_lengths else 0
            wins_last_100 = sum(self.episode_wins[-100:])  # Nombre de victoires sur les 100 derniers épisodes
            win_rate = wins_last_100 / min(len(self.episode_wins), 100)

            print(f"Step: {self.num_timesteps} ({elapsed_time:.2f}s)")
            print(f"  Score moyen (100 derniers épisodes): {avg_reward:.2f}")
            print(f"  Nombre moyen de coups joués (100 derniers épisodes): {avg_moves:.2f}")
            print(f"  Taux de victoire (100 derniers épisodes): {win_rate:.2%}")

            # Sauvegarder le meilleur modèle
            if avg_reward > self.best_reward and len(self.episode_rewards) >= 100:
                self.best_reward = avg_reward
                self.best_model_path = f"./models/sb3_dqn/best_model_{self.num_timesteps}_{avg_reward:.2f}.zip"
                self.model.save(self.best_model_path)
                print(f"  Nouveau meilleur modèle sauvegardé: {self.best_model_path}")

        return True

    def on_training_end(self):
        # Tracer les courbes d'apprentissage à la fin de l'entraînement
        if len(self.episode_rewards) > 0:
            plt.figure(figsize=(15, 5))

            # Récompenses
            plt.subplot(1, 3, 1)
            plt.plot(self.episode_rewards)
            plt.title('Récompenses par épisode')
            plt.xlabel('Épisode')
            plt.ylabel('Récompense')

            # Longueurs des épisodes
            plt.subplot(1, 3, 2)
            plt.plot(self.episode_lengths)
            plt.title('Longueur des épisodes')
            plt.xlabel('Épisode')
            plt.ylabel('Nombre d\'étapes')

            # Taux de victoire
            plt.subplot(1, 3, 3)
            window = min(100, len(self.episode_wins))
            if window > 0:
                rolling_win_rate = np.convolve(self.episode_wins, np.ones(window)/window, mode='valid')
                plt.plot(rolling_win_rate)
                plt.title(f'Taux de victoire (fenêtre: {window})')
                plt.xlabel('Épisode')
                plt.ylabel('Taux de victoire')

            plt.tight_layout()
            plt.savefig("./models/sb3_dqn/learning_curves.png")
            print("Courbes d'apprentissage sauvegardées dans ./models/sb3_dqn/learning_curves.png")

def train_sb3_dqn(episodes=10000, save_path="./models/sb3_dqn"):
    # Créer l'environnement
    env = Monitor(TacticiensEnv(opponent_type='random'))

    # Créer le modèle DQN
    model = DQN(
        "MlpPolicy",
        env,
        learning_rate=1e-3,
        buffer_size=50000,  # Augmenté pour une meilleure stabilité
        learning_starts=1000,
        batch_size=64,      # Augmenté pour un apprentissage plus efficace
        gamma=0.99,
        target_update_interval=500,
        train_freq=4,       # Réduit pour un apprentissage plus fréquent
        verbose=1
    )

    # Créer un dossier pour sauvegarder les modèles
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    # Ajouter des callbacks pour sauvegarder le modèle périodiquement et afficher des informations
    checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=save_path, name_prefix="dqn_model")
    training_info_callback = TrainingInfoCallback(verbose=1)

    # Entraîner le modèle
    start_time = time.time()
    print(f"Début de l'entraînement pour {episodes} étapes...")

    model.learn(
        total_timesteps=episodes,
        callback=[checkpoint_callback, training_info_callback],
        log_interval=1000
    )

    # Calculer le temps d'entraînement
    training_time = time.time() - start_time
    print(f"Entraînement terminé en {training_time:.2f} secondes")

    # Sauvegarder le modèle final
    final_model_path = os.path.join(save_path, "dqn_final_model")
    model.save(final_model_path)
    print(f"Modèle final sauvegardé dans {final_model_path}")

    # Fermer l'environnement
    env.close()

    return model

if __name__ == "__main__":
    # Définir les paramètres d'entraînement
    episodes = 50000  # Augmenté pour un meilleur apprentissage
    save_path = "./models/sb3_dqn"

    # Entraîner l'agent
    print(f"Démarrage de l'entraînement pour {episodes} étapes...")
    model = train_sb3_dqn(episodes, save_path)
    print("Entraînement terminé!")